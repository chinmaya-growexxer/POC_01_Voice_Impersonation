{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/growlt240/Documents/VoiceImpersonationPOC-01/poc1/lib/python3.10/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "from pinecone import Pinecone,ServerlessSpec\n",
    "from langchain_community.retrievers import PineconeHybridSearchRetriever\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain.vectorstores import Pinecone\n",
    "from langchain_groq import ChatGroq\n",
    "from faster_whisper import WhisperModel\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to connect to Pinecone DB\n",
    "def connect_to_pinecone(api_key,index_name):   \n",
    "    # Initialize pinecone client\n",
    "    pc =Pinecone(api_key=api_key)\n",
    "    # # Listing the existing indices\n",
    "    # pc.list_indexes().names()\n",
    "\n",
    "    # create index if does not exist\n",
    "    if index_name not in pc.list_indexes().names():\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=384, # dimension of dense vector\n",
    "            metric='dotproduct',# dotproduct sparse matrix \n",
    "            spec=ServerlessSpec(cloud='aws',region=\"us-east-1\")    \n",
    "        ) \n",
    "    # storing the index name for further usage\n",
    "    index = pc.Index(index_name)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve context using Pinecone\n",
    "def context_retriever(user_query,index, k=7):\n",
    "\n",
    "    # Load the model for embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "    # Load the model for sparse matrix encoding\n",
    "    bm25_encoder =BM25Encoder().default()\n",
    "\n",
    "    # Using Hybrid Search Reriever for context extraction\n",
    "    retriever = PineconeHybridSearchRetriever(embeddings=embeddings,sparse_encoder=bm25_encoder,index=index, top_k= k)\n",
    "\n",
    "    context = retriever.invoke(user_query)\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(PineconeHybridSearchRetriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install faster-whisper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process audio query\n",
    "def process_audio(audio_file_path,model):\n",
    "    segments, info = model.transcribe(audio_file_path)\n",
    "    # Combine the text from all segments\n",
    "    text = \" \".join(segment.text for segment in segments)\n",
    "    return text\n",
    " \n",
    "# # Example usage\n",
    "# audio_file_path = \"/home/growlt240/Documents/VoiceImpersonationPOC-01/Conference.wav\"\n",
    "# transcribed_text = process_audio(audio_file_path)\n",
    "# print(transcribed_text)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate LLM response\n",
    "def generate_llm_response(context, question, api_key, model_name):\n",
    "    # Use LLaMA 3 for response generation\n",
    "    llm = ChatGroq(groq_api_key=api_key, model_name= model_name, temperature= 0.2)\n",
    " \n",
    "    # Define the prompt template using PromptTemplate\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are a helpful and concise assistant. Provide detailed and accurate responses based on the given context.\n",
    "        If the answer is not available in the context, clearly state: \"The information you requested is not available in the provided context.\"\n",
    " \n",
    "        Context: {context}\n",
    " \n",
    "        Question: {question}\n",
    " \n",
    "        Answer:\n",
    "        \"\"\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    " \n",
    "    # Chain for RAG (Retrieval-Augmented Generation)\n",
    "    rag_chain = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=prompt_template\n",
    "    )\n",
    " \n",
    "    if not context:\n",
    "        # Handle case where context is empty\n",
    "        formatted_context = \"No context available.\"\n",
    "    else:\n",
    "        formatted_context = \"\\n\".join(str(item) for item in context)\n",
    "    \n",
    "    # # Define a maximum context length (e.g., 2000 characters or tokens)\n",
    "    # MAX_CONTEXT_LENGTH = 2000\n",
    "    \n",
    "    # # Truncate context if necessary\n",
    "    # if len(formatted_context) > MAX_CONTEXT_LENGTH:\n",
    "    #     formatted_context = formatted_context[:MAX_CONTEXT_LENGTH]\n",
    "    # Use 'run' instead of 'call'\n",
    "    response = rag_chain.run(context=formatted_context, question=question)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(user_query):\n",
    "    # Read the config file\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config.ini')\n",
    "\n",
    "    # Get the API details\n",
    "    api_config = config['api']\n",
    "    pinecone_api_key = api_config['pinecone_api_key']\n",
    "    groq_api_key = api_config['groq_api_key']\n",
    "\n",
    "    # required index\n",
    "    index_name = \"knowledge-base-growexx\"\n",
    "    llm_model = 'gemma2-9b-it'\n",
    "\n",
    "    # Initialize Whisper model\n",
    "    model_size = \"distil-large-v2\"\n",
    "    model = WhisperModel(model_size, device=\"cpu\", compute_type=\"float32\")\n",
    "\n",
    "    # user_query = input(\"Enter your question: \")\n",
    "\n",
    "    #Check if user_query is audio or text\n",
    "    if user_query.endswith('.wav'):\n",
    "        text_query = process_audio(user_query, model)\n",
    "    else:\n",
    "        text_query = user_query\n",
    "    \n",
    "    # Connect to knowledge base \n",
    "    index = connect_to_pinecone(pinecone_api_key,index_name)\n",
    "    \n",
    "    # Retrieve context\n",
    "    content = context_retriever(text_query,index)\n",
    "    context =[]\n",
    "    for i in content:\n",
    "        context.append(i.page_content)\n",
    "    \n",
    "    # Generate response using LLM\n",
    "    response = generate_llm_response(context, text_query, groq_api_key, llm_model)\n",
    "    return response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response generated:/nThe provided text does not contain any direct quotes or statements from clients about GrowExx. \n",
      "\n",
      "\n",
      "Therefore:  The information you requested is not available in the provided context. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_query = input(\"Enter your question: \")\n",
    "print(f\"Response generated:/n{main(user_query)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided text lists these services offered by GrowExx:\n",
      "\n",
      "* Competitive .NET development services\n",
      "* Business intelligence solutions\n",
      "* AI solutions tailored to business objectives\n",
      "* End-to-end Tableau consulting services\n",
      "* DevOps consulting services \n",
      "\n",
      "\n",
      "Let me know if you have any other questions. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# audio input\n",
    "print(main(\"/home/growlt240/Documents/VoiceImpersonationPOC-01/Record (online-voice-recorder.com).wav\"))\n",
    "#print(answer_query('what services growexx offers?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
